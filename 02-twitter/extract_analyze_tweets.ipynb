{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the tweets file in read mode\n",
    "with open('collected_tweets2020-12-07_13:29:12.txt','r') as file:\n",
    "    tweets = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse tweet to json format (list)\n",
    "parsed_tweets = [json.loads(json.loads(i)) for i in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that handles the tweet and converts it to a dataframe\n",
    "\n",
    "# Different types of tweets have different types of structure. This function only covers text tweets. Video, image etc are not treated and are therefore disregarded (try / except return None)\n",
    "\n",
    "def tweet_to_df(tweet):\n",
    "    try:\n",
    "        # Transform parsed_tweets into table format\n",
    "        df_tratado = pd.DataFrame(tweet).reset_index(drop=True).iloc[:1]\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        df_tratado.drop(columns=[\n",
    "        'quote_count'\n",
    "        ,'reply_count'\n",
    "        ,'retweet_count'\n",
    "        ,'favorite_count'\n",
    "        ,'favorited'\n",
    "        ,'retweeted'\n",
    "        ,'user'\n",
    "        ,'entities'\n",
    "        ,'retweeted_status'], inplace=True)\n",
    "\n",
    "        # Select fields from the user object and add them to the Dataframe\n",
    "        df_tratado['user_id'] = tweet['user']['id']\n",
    "        df_tratado['user_id_str'] = tweet['user']['id_str']\n",
    "        df_tratado['user_screen_name'] = tweet['user']['screen_name']\n",
    "        df_tratado['user_location'] = tweet['user']['location']\n",
    "        df_tratado['user_description'] = tweet['user']['description']\n",
    "        df_tratado['user_protected'] = tweet['user']['protected']\n",
    "        df_tratado['user_verified'] = tweet['user']['verified']\n",
    "        df_tratado['user_followers_count'] = tweet['user']['followers_count']\n",
    "        df_tratado['user_friends_count'] = tweet['user']['friends_count']\n",
    "\n",
    "        # copy(): to avoid using the pointer concept, changes made to the copied object will not affect         the original object. If we do not use copy(), making only a simple assignment, the changes              made to the new object will also be reflected in the original object.\n",
    "        user_mentions = []\n",
    "        for i in range(len(tweet['entities']['user_mentions'])):\n",
    "            dict_base = tweet['entities']['user_mentions'][i].copy()\n",
    "            dict_base.pop('indices', None)\n",
    "            df = pd.DataFrame(dict_base, index=[0])\n",
    "            df = df.rename(columns={\n",
    "                'screen_name':'entities_screen_name'\n",
    "                ,'name':'entities_name'\n",
    "                ,'id':'entities_id'\n",
    "                ,'id_str':'entities_id_str'\n",
    "            })\n",
    "            user_mentions.append(df)\n",
    "\n",
    "        dfs = []\n",
    "        for i in user_mentions:\n",
    "            dfs.append(\n",
    "                pd.concat([df_tratado.copy(), i], axis=1)\n",
    "            )\n",
    "\n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "    except:\n",
    "        return None\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3min 11s, sys: 293 ms, total: 3min 12s\nWall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time # runtime\n",
    "# Iterate and transform all tweets\n",
    "# We do the iteration saving it in a list because iterating over DataFrame in Python is exponentially slow.\n",
    "parsed = [tweet_to_df(tweet) for tweet in parsed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes empty positions from the list (those non-text tweets and the function returns none)\n",
    "parsed = [i for i in parsed if i is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat(parsed, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database params\n",
    "path = os.path.join('config.ini')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(path)\n",
    "params = 'postgresql://{}:{}@{}:{}/{}'.format(config.get('postgresql_db', 'user')\n",
    "                                              ,config.get('postgresql_db', 'pass')                                                                    ,config.get('postgresql_db','server')\n",
    "                                              ,config.get('postgresql_db', 'port')\n",
    "                                              ,config.get('postgresql_db', 'db'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to posgresql\n",
    "engine = create_engine(params)\n",
    "final_data.to_sql('text_tweets', engine, index=False, if_exists=\"append\", schema='public')"
   ]
  }
 ]
}